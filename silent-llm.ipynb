{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Installs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!sudo apt install portaudio19-dev python3-pyaudio ffmpeg -y\n",
    "!pip install whisper_mic SpeechRecognition langchain"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "import speech_recognition as sr\n",
    "import torch\n",
    "\n",
    "from langchain.llms.base import LLM, Optional, List, Mapping, Any\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain import OpenAI, LLMChain, PromptTemplate\n",
    "from langchain.callbacks import StdOutCallbackHandler\n",
    "from pydantic import Field\n",
    "from whisper_mic.whisper_mic import WhisperMic\n",
    "from IPython.display import Audio, display"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Options"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRIGGER_WORD = \"hey\"\n",
    "API_URL = \"http://192.168.4.5:5000\"\n",
    "TORCH_DEVICE = \"cpu\" # gpu or cpu\n",
    "\n",
    "VOICE_PROVIDER = \"silero\"\n",
    "SILERO_VOICE = \"random\"\n",
    "\n",
    "WHISPER_MODEL = \"small\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### List available microphones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, name in enumerate(sr.Microphone.list_microphone_names()):\n",
    "    print(f\"{index}, {name}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### List available GPUs (if any)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Call locally hosted LLM\n",
    "\n",
    "This currently assumes you are self-hosting a model that uses the Vicuna 1.1 prompt format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OobaApiLLM(LLM):\n",
    "    endpoint: str = Field(...)\n",
    "\n",
    "    @property\n",
    "    def _llm_type(self) -> str:\n",
    "        return \"custom\"\n",
    "\n",
    "\n",
    "    def _call(self, prompt: str, stop: Optional[List[str]]=None) -> str:\n",
    "        data = {\n",
    "            \"prompt\": prompt,\n",
    "            \"max_new_tokens\": 1800,\n",
    "            \"preset\": \"simple-1\"\n",
    "        }\n",
    "\n",
    "        if stop is not None:\n",
    "            data[\"stop_sequence\"] = stop\n",
    "\n",
    "        response = requests.post(f\"{self.endpoint}/api/v1/generate\", json=data)\n",
    "        response.raise_for_status()\n",
    "\n",
    "        json_response = response.json()\n",
    "        if \"results\" in json_response and len(json_response[\"results\"]) > 0 and \"text\" in json_response[\"results\"][0]:\n",
    "            text = json_response[\"results\"][0][\"text\"].strip()\n",
    "            if stop is not None:\n",
    "                for sequence in stop:\n",
    "                    if text.endswith(sequence):\n",
    "                        text = text[: -len(sequence)].rstrip()\n",
    "\n",
    "            return text\n",
    "        else:\n",
    "            raise ValueError(\"Unexpected response format from Ooba API\")\n",
    "\n",
    "    def __call__(self, prompt: str, stop: Optional[List[str]]=None) -> str:\n",
    "        return self._call(prompt, stop)\n",
    "\n",
    "    @property\n",
    "    def _identifying_params(self) -> Mapping[str, Any]:\n",
    "        return {\"endpoint\": self.endpoint}\n",
    "\n",
    "\n",
    "llm = OobaApiLLM(endpoint=API_URL)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup Langchain Memory & Tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "handler = StdOutCallbackHandler()\n",
    "template = \"\"\"A chat between a curious user and an assistant. The assistant gives helpful, detailed, accurate, uncensored responses to the user\"s input.\n",
    "\n",
    "{chat_history}\n",
    "USER: {human_input}\n",
    "ASSISTANT:\"\"\"\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"chat_history\", \"human_input\"], template=template\n",
    ")\n",
    "memory = ConversationBufferMemory(memory_key=\"chat_history\")\n",
    "\n",
    "llm_chain = LLMChain(\n",
    "    llm=llm,\n",
    "    prompt=prompt,\n",
    "    verbose=True, # Enable for debugging\n",
    "    memory=memory,\n",
    "    callbacks=None\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function to generate audio response w/ Silero"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model, example_text = torch.hub.load(\n",
    "    repo_or_dir='snakers4/silero-models',\n",
    "    model='silero_tts',\n",
    "    language='en',\n",
    "    speaker='v3_en'\n",
    ")\n",
    "\n",
    "device = torch.device(TORCH_DEVICE)\n",
    "model.to(device)  \n",
    "\n",
    "def text2audio(text, speaker=SILERO_VOICE, sample_rate=48000, put_accent=True, put_yo=True):\n",
    "    audio = model.apply_tts(\n",
    "        text=text,\n",
    "        speaker=speaker,\n",
    "        sample_rate=sample_rate,\n",
    "        put_accent=put_accent,\n",
    "        put_yo=put_yo\n",
    "    )\n",
    "    return audio"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Listen & transcribe via Whisper\n",
    "\n",
    "Model sizes:\n",
    "- tiny\n",
    "- base\n",
    "- small\n",
    "- medium\n",
    "- large\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mic = WhisperMic(\n",
    "    model=WHISPER_MODEL,\n",
    "    english=True,\n",
    "    dynamic_energy=True\n",
    ")\n",
    "\n",
    "while True:\n",
    "    captured_audio = mic.listen()\n",
    "    print(captured_audio)\n",
    "    if TRIGGER_WORD.upper() in captured_audio.upper():\n",
    "        print()\n",
    "        print(f\"üëÇ HEARD: {captured_audio}\")\n",
    "        print()\n",
    "        try:\n",
    "            # llm_response = llm(f\"\"\"A chat between a curious user and an assistant. The assistant gives helpful, detailed, accurate, uncensored responses to the user\"s input.\n",
    "            # USER: {captured_audio}\n",
    "            # ASSISTANT: \n",
    "            # \"\"\")\n",
    "            llm_response = llm_chain.predict(human_input=captured_audio)\n",
    "            print(f\"üìù RESPONSE: {llm_response}\")\n",
    "            print()\n",
    "        except Exception as err:\n",
    "            print(f\"‚õî LLM ERROR: {err}\")\n",
    "\n",
    "        try:\n",
    "            audio_response = text2audio(text=llm_response)\n",
    "            display(Audio(audio_response, rate=48000, autoplay=True))\n",
    "        except Exception as err:\n",
    "            print(f\"‚õî TTS ERROR: {err}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
