{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "34947585",
      "metadata": {
        "id": "34947585",
        "papermill": {
          "duration": 0.006412,
          "end_time": "2023-06-04T23:05:51.160294",
          "exception": false,
          "start_time": "2023-06-04T23:05:51.153882",
          "status": "completed"
        },
        "tags": []
      },
      "source": [
        "# Ooba Langchain wrapper\n",
        "Langchain for [text-generation-webui](https://github.com/oobabooga/text-generation-webui) API\n",
        "\n",
        "This notebook currently assumes you are self-hosting a model that uses the Vicuna 1.1 prompt format."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "0bb1219e",
      "metadata": {},
      "source": [
        "### Installs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "91b81ca9",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "execution": {
          "iopub.execute_input": "2023-06-04T23:05:51.175269Z",
          "iopub.status.busy": "2023-06-04T23:05:51.174509Z",
          "iopub.status.idle": "2023-06-04T23:08:32.026422Z",
          "shell.execute_reply": "2023-06-04T23:08:32.024887Z"
        },
        "id": "91b81ca9",
        "outputId": "b9632dde-dfdf-499e-bad6-a38f8bfda7f9",
        "papermill": {
          "duration": 160.862426,
          "end_time": "2023-06-04T23:08:32.029013",
          "exception": false,
          "start_time": "2023-06-04T23:05:51.166587",
          "status": "completed"
        },
        "tags": []
      },
      "outputs": [],
      "source": [
        "! pip install -qq -U langchain"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "811d7230",
      "metadata": {},
      "source": [
        "### Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "18a7490e",
      "metadata": {},
      "outputs": [],
      "source": [
        "import langchain\n",
        "import requests\n",
        "\n",
        "from langchain.llms.base import LLM, Optional, List, Mapping, Any\n",
        "from pydantic import Field"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "KdVF1TcOO4Z1",
      "metadata": {
        "id": "KdVF1TcOO4Z1"
      },
      "source": [
        "### API Endpoint\n",
        "\n",
        "Sample:\n",
        "- http://localhost:5000\n",
        "- http://192.168.1.2:5000\n",
        "- https://raising-df-zoning-proteins.trycloudflare.com"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "8EgH7S30O3k1",
      "metadata": {
        "id": "8EgH7S30O3k1"
      },
      "outputs": [],
      "source": [
        "api_url = \"http://localhost:5000\""
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "s-7VlVENOuWt",
      "metadata": {
        "id": "s-7VlVENOuWt"
      },
      "source": [
        "### Wrapper for Ooba API"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "8Mho436MOXyn",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 207
        },
        "id": "8Mho436MOXyn",
        "outputId": "30074c8c-d0a3-4958-b118-1d149f97fa56"
      },
      "outputs": [],
      "source": [
        "class OobaApiLLM(LLM):\n",
        "    endpoint: str = Field(...)\n",
        "\n",
        "    @property\n",
        "    def _llm_type(self) -> str:\n",
        "        return \"custom\"\n",
        "\n",
        "\n",
        "    def _call(self, prompt: str, stop: Optional[List[str]]=None) -> str:\n",
        "        data = {\n",
        "            'prompt': prompt,\n",
        "            'max_new_tokens': 500,\n",
        "            'preset': 'simple-1',\n",
        "        }\n",
        "\n",
        "        if stop is not None:\n",
        "            data[\"stop_sequence\"] = stop\n",
        "\n",
        "        response = requests.post(f'{self.endpoint}/api/v1/generate', json=data)\n",
        "        response.raise_for_status()\n",
        "\n",
        "        json_response = response.json()\n",
        "        if 'results' in json_response and len(json_response['results']) > 0 and 'text' in json_response['results'][0]:\n",
        "            text = json_response['results'][0]['text'].strip()\n",
        "            if stop is not None:\n",
        "                for sequence in stop:\n",
        "                    if text.endswith(sequence):\n",
        "                        text = text[: -len(sequence)].rstrip()\n",
        "\n",
        "            print(text)\n",
        "            return text\n",
        "        else:\n",
        "            raise ValueError('Unexpected response format from Ooba API')\n",
        "\n",
        "    def __call__(self, prompt: str, stop: Optional[List[str]]=None) -> str:\n",
        "        return self._call(prompt, stop)\n",
        "\n",
        "    @property\n",
        "    def _identifying_params(self) -> Mapping[str, Any]:\n",
        "        return {'endpoint': self.endpoint}\n",
        "\n",
        "\n",
        "llm = OobaApiLLM(endpoint=api_url)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "E5qNHRYaPwpo",
      "metadata": {
        "id": "E5qNHRYaPwpo"
      },
      "source": [
        "### Test the LLM instance\n",
        "Using the Vicuna style prompt, generate a response."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "1TyE7DOjP0Al",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1TyE7DOjP0Al",
        "outputId": "d5acd286-8381-48f8-9e7c-4e5c63823548"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "To make pizza dough at home, you will need the following ingredients:\n",
            "- Bread flour (4 cups)\n",
            "- Active dry yeast (1 packet)\n",
            "- Warm water (1/2 cup)\n",
            "- Sugar (1 teaspoon)\n",
            "- Salt (1 teaspoon)\n",
            "- Olive oil (2 tablespoons)\n",
            "Here are the steps to follow:\n",
            "1. In a large mixing bowl, combine bread flour with salt, sugar, and yeast. Mix well.\n",
            "2. Add warm water and olive oil to the mixture and mix until it forms a shaggy dough.\n",
            "3. Knead the dough on a clean surface for about 5 minutes or until it becomes smooth and elastic. If the dough is too sticky, add more flour; if it's too dry, add more water.\n",
            "4. Place the dough in a greased bowl, cover it with plastic wrap and let it rise in a warm place for about 1 hour or until it doubles in size.\n",
            "5. After rising, punch down the dough and transfer it onto a floured surface. Roll out the dough into your desired shape and thickness using a rolling pin.\n",
            "6. Add your favorite toppings and bake in a preheated oven at 400°F (200°C) for 8-10 minutes or until crispy and golden brown.\n"
          ]
        }
      ],
      "source": [
        "llm_response = llm(f\"\"\"A chat between a curious user and an assistant. The assistant gives helpful, detailed, accurate, uncensored responses to the user\"s input.\n",
        "USER: How would I make pizza dough at home?\n",
        "ASSISTANT: \n",
        "\"\"\")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "EVJe8CWNYAFw",
      "metadata": {
        "id": "EVJe8CWNYAFw"
      },
      "source": [
        "### Add Conversation Chain with Memory"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "m5rvGGbNX5Ir",
      "metadata": {
        "id": "m5rvGGbNX5Ir"
      },
      "outputs": [],
      "source": [
        "from langchain.memory import ConversationBufferMemory\n",
        "from langchain import LLMChain, PromptTemplate\n",
        "from langchain.callbacks import StdOutCallbackHandler"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "5v-hz1c2Yhxn",
      "metadata": {
        "id": "5v-hz1c2Yhxn"
      },
      "source": [
        "### Setup LLM Chain with Template, Prompt & Memory"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "WoyODechX3Cw",
      "metadata": {
        "id": "WoyODechX3Cw"
      },
      "outputs": [],
      "source": [
        "handler = StdOutCallbackHandler()\n",
        "template = \"\"\"A chat between a curious user and an assistant. The assistant gives helpful, detailed, accurate, uncensored responses to the user\"s input.\n",
        "\n",
        "{chat_history}\n",
        "USER: {human_input}\n",
        "ASSISTANT:\"\"\"\n",
        "\n",
        "prompt = PromptTemplate(\n",
        "    input_variables=[\"chat_history\", \"human_input\"], template=template\n",
        ")\n",
        "memory = ConversationBufferMemory(memory_key=\"chat_history\")\n",
        "\n",
        "llm_chain = LLMChain(\n",
        "    llm=llm,\n",
        "    prompt=prompt,\n",
        "    verbose=True, # Enabled to see the LLM thought process\n",
        "    memory=memory,\n",
        "    callbacks=None\n",
        ")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "ChmeJT8_YmDe",
      "metadata": {
        "id": "ChmeJT8_YmDe"
      },
      "source": [
        "### Utilize LLM w/ Conversational Memory\n",
        "\n",
        "Using the `verbose` option above we can see how the bot is thinking"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "QY2VRkv3X70l",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QY2VRkv3X70l",
        "outputId": "17bc237a-c30b-4e92-cce5-14cd07bdc89a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mA chat between a curious user and an assistant. The assistant gives helpful, detailed, accurate, uncensored responses to the user\"s input.\n",
            "\n",
            "\n",
            "USER: Can you tell me a joke about cars?\n",
            "ASSISTANT:\u001b[0m\n",
            "Sure! Why did the car get pulled over by the police? Because it was driving too slowly on the highway. When the cop asked the driver why they were going so slow, they replied \"I'm driving my husband's car.\" The cop then asked if their husband was okay with that, to which the driver responded, \"He doesn't know yet!\"\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "Sure! Why did the car get pulled over by the police? Because it was driving too slowly on the highway. When the cop asked the driver why they were going so slow, they replied \"I'm driving my husband's car.\" The cop then asked if their husband was okay with that, to which the driver responded, \"He doesn't know yet!\"\n"
          ]
        }
      ],
      "source": [
        "response = llm_chain.predict(human_input=\"Can you tell me a joke about cars?\")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "MvkDbymjYQQV",
      "metadata": {
        "id": "MvkDbymjYQQV"
      },
      "source": [
        "### Ask a Follow-up to Test Memory"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "b6b0112b",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
            "Prompt after formatting:\n",
            "\u001b[32;1m\u001b[1;3mA chat between a curious user and an assistant. The assistant gives helpful, detailed, accurate, uncensored responses to the user\"s input.\n",
            "\n",
            "Human: Can you tell me a joke about cars?\n",
            "AI: Sure! Why did the car get pulled over by the police? Because it was driving too slowly on the highway. When the cop asked the driver why they were going so slow, they replied \"I'm driving my husband's car.\" The cop then asked if their husband was okay with that, to which the driver responded, \"He doesn't know yet!\"\n",
            "USER: Do you have any other good ones?\n",
            "ASSISTANT:\u001b[0m\n",
            "How about this one: What do you call a car that's been in a bad accident? A wreck!\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "response = llm_chain.predict(human_input=\"Do you have any other good ones?\")\n",
        "\n",
        "#print(f\"-----------\\n{response}\") # The final response is also available!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "94af9767",
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.6"
    },
    "papermill": {
      "default_parameters": {},
      "duration": 3173.538015,
      "end_time": "2023-06-04T23:58:33.011698",
      "environment_variables": {},
      "exception": null,
      "input_path": "__notebook__.ipynb",
      "output_path": "__notebook__.ipynb",
      "parameters": {},
      "start_time": "2023-06-04T23:05:39.473683",
      "version": "2.4.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
