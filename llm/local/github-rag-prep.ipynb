{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ingest GitHub Repos for RAG\n",
    "\n",
    "**R.A.G.** -- Retrieval Augmented Generation\n",
    "\n",
    "<img src=\"https://docs.llamaindex.ai/en/stable/_images/basic_rag.png\" width=\"50%\">\n",
    "\n",
    "Resources:\n",
    "- [llama_index](https://docs.llamaindex.ai/en/stable/index.html) - Data Embedding\n",
    "- [Ollama](https://ollama.ai/) - Local LLM Wrapper"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "USERNAME = 'fairbanksio'\n",
    "MODEL = 'llama2'\n",
    "OLLAMA_URL = 'http://192.168.4.5:11434'"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Installs & Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -q -U python-dotenv psycopg2-binary hvac llama-index torch transformers python-pptx Pillow pypdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import requests\n",
    "import hvac\n",
    "import os\n",
    "import warnings\n",
    "import subprocess\n",
    "import os\n",
    "import psycopg2\n",
    "import shutil\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "from llama_index import SimpleDirectoryReader, VectorStoreIndex, ServiceContext, set_global_service_context\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "# Disable all warnings\n",
    "warnings.simplefilter(\"ignore\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set a fake OpenAI API Key \n",
    "\n",
    "DO NOT CHANGE -- `llama_index` checks for this to be set but it is not used/needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"OPENAI_API_KEY\"] = \"sk-abc123\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Connect to Vault"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "client = hvac.Client(\n",
    "    url=os.getenv('VAULT_API'),\n",
    "    token=os.getenv('VAULT_TOKEN'),\n",
    ")\n",
    "\n",
    "print(client.is_authenticated())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Get GitHub Secrets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    secret_resp = client.secrets.kv.v2.read_secret_version(\n",
    "        mount_point='kv', \n",
    "        path='github', \n",
    "        raise_on_deleted_version=False\n",
    "    )\n",
    "    \n",
    "    if secret_resp['data'] is not None:\n",
    "        secret_values = secret_resp['data']['data']\n",
    "        for secret, value in secret_values.items():\n",
    "            os.environ[str(secret)] = str(value)\n",
    "    else:\n",
    "        print(\"The secret does not exist.\")\n",
    "except hvac.exceptions.InvalidPath:\n",
    "    print(\"The path is invalid or the permission is denied.\")\n",
    "except hvac.exceptions.Forbidden:\n",
    "    print(\"The permission is denied.\")\n",
    "except hvac.exceptions.VaultError as e:\n",
    "    print(f\"Vault error occurred: {e}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Get Postgres Secrets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    secret_resp = client.secrets.kv.v2.read_secret_version(\n",
    "        mount_point='kv', \n",
    "        path='postgres', \n",
    "        raise_on_deleted_version=False\n",
    "    )\n",
    "    \n",
    "    if secret_resp['data'] is not None:\n",
    "        secret_values = secret_resp['data']['data']\n",
    "        for secret, value in secret_values.items():\n",
    "            os.environ[str(secret)] = str(value)\n",
    "    else:\n",
    "        print(\"The secret does not exist.\")\n",
    "except hvac.exceptions.InvalidPath:\n",
    "    print(\"The path is invalid or the permission is denied.\")\n",
    "except hvac.exceptions.Forbidden:\n",
    "    print(\"The permission is denied.\")\n",
    "except hvac.exceptions.VaultError as e:\n",
    "    print(f\"Vault error occurred: {e}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scrape GitHub Repos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_github_repos(username, token):\n",
    "    url = f'https://api.github.com/users/{username}/repos'\n",
    "    headers = {'Authorization': f'token {token}'}\n",
    "    \n",
    "    all_repos = []\n",
    "    page = 1\n",
    "\n",
    "    while True:\n",
    "        params = {'page': page, 'per_page': 100}\n",
    "        response = requests.get(url, headers=headers, params=params)\n",
    "\n",
    "        if response.status_code == 200:\n",
    "            repos = response.json()\n",
    "            if not repos:\n",
    "                break  # No more repositories\n",
    "            all_repos.extend(repos)\n",
    "            page += 1\n",
    "        else:\n",
    "            print(f\"Error fetching repositories. Status code: {response.status_code}\")\n",
    "            return None\n",
    "\n",
    "    repo_names = [repo['name'] for repo in all_repos if not repo['fork']]\n",
    "    return repo_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_to_json(data, filename):\n",
    "    with open(filename, 'w') as json_file:\n",
    "        json.dump(data, json_file, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 35 GitHub Repositories created by fairbanksio:\n",
      "\n",
      "Blink\n",
      "digital-ocean-billing-paypal\n",
      "ExpressAPI\n",
      "ExpressGQL\n",
      "f5-client\n",
      "f5-fetch-data\n",
      "f5-get-posts\n",
      "f5oclock\n",
      "Factorio-Mod-Updater\n",
      "flux-gitops-apps\n",
      "GoHTTP\n",
      "helm-charts\n",
      "jenkins-docker\n",
      "Jetson-Camera\n",
      "k6\n",
      "kube-cluster\n",
      "next-skeleton\n",
      "notebooks\n",
      "PayPal-IPN-Listener\n",
      "PayPal-Payment-Generator\n",
      "PayPal-Sandbox-Dashboard\n",
      "react-register\n",
      "react-skeleton\n",
      "Roadrunner\n",
      "rtsp-nvr\n",
      "site-status\n",
      "slack-history-export\n",
      "Spot\n",
      "tf-iac-apps\n",
      "tf-iac-cluster\n",
      "tf-iac-demo\n",
      "tf-iac-infra\n",
      "tiles-api\n",
      "tiles-client\n",
      "uptime-monitor\n",
      "\n",
      "Repository list saved to 'fairbanksio_repos.json'\n"
     ]
    }
   ],
   "source": [
    "REPOS = get_github_repos(USERNAME, os.getenv(\"PERSONAL_TOKEN\"))\n",
    "\n",
    "if REPOS:\n",
    "    print(f\"Found {len(REPOS)} GitHub Repositories created by {USERNAME}:\\n\")\n",
    "    for repo in REPOS:\n",
    "        print(f\"{repo}\")\n",
    "\n",
    "    # Save to JSON file with username as filename\n",
    "    save_to_json(REPOS, f'{USERNAME}_repos.json')\n",
    "    print(f\"\\nRepository list saved to '{USERNAME}_repos.json'\")\n",
    "else:\n",
    "    print(f\"Unable to fetch GitHub repositories for {USERNAME}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clone Repos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clone_repos(repos, target_folder):\n",
    "    for repo in repos:\n",
    "        clone_command = f\"git clone -q https://github.com/{USERNAME}/{repo}.git {target_folder}/{repo}\"\n",
    "        try:\n",
    "            subprocess.run(clone_command, shell=True)\n",
    "        except Exception:\n",
    "            continue"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Set where repos should be cloned temporarily for conversion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/jovyan/temp/fairbanksio\n"
     ]
    }
   ],
   "source": [
    "REPO_DIRECTORY = os.getcwd() + \"/temp/\" + USERNAME\n",
    "print(REPO_DIRECTORY)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Start the Cloning Process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "clone_repos(REPOS, REPO_DIRECTORY)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Connect to Postgres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Database Cursor: Ready\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    connection = psycopg2.connect(\n",
    "        host=os.getenv(\"POSTGRES_HOST\"),\n",
    "        port=os.getenv(\"POSTGRES_POST\"),\n",
    "        dbname=os.getenv(\"POSTGRES_DBNAME\"),\n",
    "        user=os.getenv(\"POSTGRES_USER\"),\n",
    "        password=os.getenv(\"POSTGRES_PASSWORD\")\n",
    "    )\n",
    "\n",
    "    # Create a cursor object to interact with the database\n",
    "    cursor = connection.cursor()\n",
    "\n",
    "    if cursor:\n",
    "        print(\"Database Cursor: Ready\")\n",
    "\n",
    "except psycopg2.Error as err:\n",
    "    print(f\"Database Error: {err}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup Ollama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello there! It's nice to meet you. How are you today? Is there something I can help you with or would you like to chat?\n"
     ]
    }
   ],
   "source": [
    "from llama_index.llms import Ollama\n",
    "\n",
    "llm = Ollama(\n",
    "    model=MODEL,\n",
    "    base_url=OLLAMA_URL,\n",
    "    request_timeout=60.0\n",
    ")\n",
    "\n",
    "resp = llm.complete(\"Hello World\")\n",
    "print(resp)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Files and Create Search Index"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configuration for Local Embeddings & Local LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "service_context = ServiceContext.from_defaults(\n",
    "    embed_model=\"local:BAAI/bge-large-en\",\n",
    "    chunk_size=1024,\n",
    "    llm=llm\n",
    ")\n",
    "\n",
    "set_global_service_context(service_context)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Files from Directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading files: 100%|██████████| 1537/1537 [00:14<00:00, 103.70file/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 1,780 docs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "files = SimpleDirectoryReader(input_dir=REPO_DIRECTORY, recursive=True)\n",
    "documents = files.load_data(show_progress=True)\n",
    "print(f\"Loaded {len(documents):,} documents\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build a File Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8bd3719fb2f7465d9e72859da77e937c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Parsing nodes:   0%|          | 0/1780 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "80618c640dbb4c068d00ef81e3ba0a05",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating embeddings:   0%|          | 0/2048 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c50728951ede4d758b407bee58990e3c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating embeddings:   0%|          | 0/2048 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e0c581fb6ca4459091437e3a4b9ef1fd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating embeddings:   0%|          | 0/2048 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "373e267ff7434df0bc814aa9a25837b0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating embeddings:   0%|          | 0/2048 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "21909d05d21241a092ab96baebcf5424",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating embeddings:   0%|          | 0/2048 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "863ef1604c7042aabaaba42ce20c9288",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating embeddings:   0%|          | 0/2048 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "00253d09073142e3851ee075ed5cd1e8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating embeddings:   0%|          | 0/2048 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "702d0762d57c44bebe7b981ba059005e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating embeddings:   0%|          | 0/398 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "index = VectorStoreIndex.from_documents(\n",
    "    documents, \n",
    "    service_context=service_context,\n",
    "    show_progress=True\n",
    ")\n",
    "query_engine = index.as_query_engine()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove Temp Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully removed /home/jovyan/temp/jonfairbanks\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    shutil.rmtree(REPO_DIRECTORY)\n",
    "    print(f\"Successfully removed {REPO_DIRECTORY}\")\n",
    "except OSError as e:\n",
    "    print(f\"Error: {e}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Query the Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Yes, there are several improvements that could be made to the Yo codebase. Here are some suggestions based on the provided context information:\n",
      "\n",
      "1. **Modularize the code**: The current codebase is quite monolithic, with many functions and variables scattered throughout the file. Modularizing the code would make it easier to maintain and update in the future. One approach could be to create separate modules for different components of the Yo system, such as the server, client, and database layers.\n",
      "2. **Use TypeScript**: TypeScript is a superset of JavaScript that adds static typing and other features. Using TypeScript could help catch errors earlier in the development process and make the codebase more maintainable.\n",
      "3. **Implement proper error handling**: The current codebase does not have comprehensive error handling, which can lead to unexpected behavior when encountering errors. Implementing proper error handling mechanisms, such as error objects or middleware functions, would help handle errors in a more robust and predictable manner.\n",
      "4. **Use design patterns**: The Yo system is quite complex, and there are opportunities to use various design patterns to improve its maintainability and scalability. For example, using the Factory pattern could simplify the client initialization process, while the Singleton pattern could help ensure consistency across different parts of the system.\n",
      "5. **Optimize database queries**: The current codebase uses raw SQL queries for interacting with the MongoDB database. Optimizing these queries using tools like Mongoose or by writing more efficient queries could improve performance and reduce the risk of slow queries.\n",
      "6. **Implement authentication and authorization**: While the current codebase has basic authentication mechanisms, it does not have a comprehensive authorization system. Implementing proper authentication and authorization layers could help ensure that only authorized users can access sensitive parts of the system.\n",
      "7. **Use observability tools**: The Yo system does not have extensive logging or monitoring mechanisms. Implementing observability tools like Prometheus, Grafana, or Elasticsearch could help monitor the system's performance and identify potential issues before they become critical.\n",
      "8. **Improve performance**: The current codebase could benefit from various performance optimization techniques, such as memoization, caching, or using more efficient algorithms for handling large datasets.\n",
      "9. **Use modern web development practices**: The current codebase uses a mix of modern and older technologies, which can lead to compatibility issues or increased maintenance effort. Updating the codebase to use only modern technologies and practices could simplify maintenance and improve overall performance.\n",
      "10. **Implement continuous integration and deployment**: Setting up a proper continuous integration and deployment (CI/CD) pipeline could help automate the testing, building, and deployment process for the Yo system, making it easier to maintain and update in the long run.\n"
     ]
    }
   ],
   "source": [
    "query_engine = index.as_query_engine(\n",
    "    similarity_top_k=5, # Return additional results\n",
    "    service_context=service_context\n",
    ") \n",
    "response = query_engine.query(\"Are there any improvements that could be made to the Yo codebase?\")\n",
    "print(response)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Index Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Execute a SQL query\n",
    "# cursor.execute(\"SELECT version();\")\n",
    "\n",
    "# Fetch the result\n",
    "# result = cursor.fetchone()\n",
    "# print(\"PostgreSQL version:\", result)\n",
    "\n",
    "# if connection:\n",
    "#     cursor.close()\n",
    "#     connection.close()\n",
    "#     print(\"Connection closed.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
