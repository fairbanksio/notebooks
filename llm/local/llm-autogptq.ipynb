{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run Large Language Model from Python"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Models & Quantization\n",
    "\n",
    "Most large language models, depending on the number of parameters, can be prohibitively large (>24GB).\n",
    "\n",
    "To allow these models to run locally on consumer grade GPUs, they can be quantized into various bit sizes using a few different methods. \n",
    "\n",
    "- GPTQ: All model layers are loaded into VRAM and GPU is used for inference. Best for fast performance.\n",
    "- GGUF: Successor to GGML. Inference is done via CPU + RAM. Model layers can optionally be loaded into VRAM.\n",
    "- AWQ: New, GPTQ like method which offers 4-bit quantization at fast speeds with up to 3x less memory utlization. \n",
    "\n",
    "_Note: AutoAWQ does not yet support Mixtral_"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q -U optimum huggingface-hub auto-gptq\n",
    "!pip uninstall transformers -y\n",
    "!pip install -q -U git+https://github.com/huggingface/transformers"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Install Model(s)\n",
    "\n",
    "Sample Models\n",
    "- [Mixtral 8x7B GPTQ](https://huggingface.co/TheBloke/Mixtral-8x7B-v0.1-GPTQ)\n",
    "- [CodeLlama-34B-Instruct-GPTQ](https://huggingface.co/TheBloke/CodeLlama-34B-Instruct-GPTQ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!sudo mkdir -p TheBloke/CodeLlama-34B-Instruct-GPTQ\n",
    "!sudo huggingface-cli download TheBloke/CodeLlama-34B-Instruct-GPTQ --local-dir /TheBloke/CodeLlama-34B-Instruct-GPTQ --local-dir-use-symlinks False\n",
    "!ls\n",
    "!ls -lhr /TheBloke/CodeLlama-34B-Instruct-GPTQ"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
    "\n",
    "model_name_or_path = \"TheBloke/CodeLlama-34B-Instruct-GPTQ\"\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name_or_path,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=False,\n",
    "    revision=\"main\"\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name_or_path, use_fast=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"Tell me about AI\"\n",
    "prompt_template=f'''[INST] Write code to solve the following coding problem that obeys the constraints and passes the example test cases. Please wrap your code answer using ```:\n",
    "{prompt}\n",
    "[/INST]\n",
    "\n",
    "'''\n",
    "\n",
    "print(\"\\n\\n*** Generate:\")\n",
    "\n",
    "input_ids = tokenizer(prompt_template, return_tensors='pt').input_ids.cuda()\n",
    "output = model.generate(\n",
    "    inputs=input_ids, \n",
    "    temperature=0.7, \n",
    "    do_sample=True, \n",
    "    top_p=0.95, \n",
    "    top_k=40, \n",
    "    max_new_tokens=512\n",
    ")\n",
    "\n",
    "print(tokenizer.decode(output[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inference can also be done using transformers' pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"*** Pipeline:\")\n",
    "pipe = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    max_new_tokens=512,\n",
    "    do_sample=True,\n",
    "    temperature=0.7,\n",
    "    top_p=0.95,\n",
    "    top_k=40,\n",
    "    repetition_penalty=1.1\n",
    ")\n",
    "\n",
    "print(pipe(prompt_template)[0]['generated_text'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
