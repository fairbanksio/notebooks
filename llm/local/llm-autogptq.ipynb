{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run Large Language Model in Python"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Models & Quantization\n",
    "\n",
    "Most large language models, depending on the number of parameters, can be prohibitively large (>24GB).\n",
    "\n",
    "To allow these models to run locally on consumer grade GPUs, they can be quantized into various bit sizes using a few different methods. \n",
    "\n",
    "- GPTQ: All model layers are loaded into VRAM and GPU is used for inference. Best for fast performance.\n",
    "- GGUF: Successor to GGML. Inference is done via CPU + RAM. Model layers can optionally be loaded into VRAM.\n",
    "- AWQ: New, GPTQ like method which offers 4-bit quantization at fast speeds with up to 3x less memory utlization. \n",
    "\n",
    "_Note: AutoAWQ does not yet support Mixtral_"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -qq -U autoawq optimum huggingface-hub\n",
    "!pip install -qq -U auto-gptq --extra-index-url https://huggingface.github.io/autogptq-index/whl/cu118/"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Install Model(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 23G\n",
      "-rwxr-xr-x 1 jovyan jovyan 482K Dec 25 23:31 tokenizer.model\n",
      "-rwxr-xr-x 1 jovyan jovyan 1.8M Dec 25 23:31 tokenizer.json\n",
      "-rwxr-xr-x 1 jovyan jovyan  967 Dec 25 23:31 tokenizer_config.json\n",
      "-rwxr-xr-x 1 jovyan jovyan   72 Dec 25 23:31 special_tokens_map.json\n",
      "-rwxr-xr-x 1 jovyan jovyan  22K Dec 25 23:31 README.md\n",
      "-rwxr-xr-x 1 jovyan jovyan  185 Dec 25 23:31 quantize_config.json\n",
      "-rwxr-xr-x 1 jovyan jovyan  23G Dec 25 23:37 model.safetensors\n",
      "-rwxr-xr-x 1 jovyan jovyan  116 Dec 25 23:31 generation_config.json\n",
      "-rwxr-xr-x 1 jovyan jovyan 2.2K Dec 25 23:31 config.json\n"
     ]
    }
   ],
   "source": [
    "!sudo mkdir -p /models/Mixtral-8x7B-v0.1-GPTQ\n",
    "!huggingface-cli download TheBloke/Mixtral-8x7B-v0.1-GPTQ --local-dir /models/Mixtral-8x7B-v0.1-GPTQ --local-dir-use-symlinks False\n",
    "!ls -lhr models/Mixtral-8x7B-v0.1-GPTQ"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-26 00:09:03.294361: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 AVX512F AVX512_VNNI AVX512_BF16 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-12-26 00:09:03.410342: I tensorflow/core/util/port.cc:104] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fdcfebd6f05e4b5191dd1aec6d176ada",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/2.21k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in /home/jovyan/.cache/huggingface/hub/models--TheBloke--Mixtral-8x7B-v0.1-GPTQ. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "050b02c8f3844537a0d41b057874e9f7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/23.8G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
    "\n",
    "model_name_or_path = \"TheBloke/Mixtral-8x7B-v0.1-GPTQ\"\n",
    "# To use a different branch, change revision\n",
    "# For example: revision=\"gptq-4bit-128g-actorder_True\"\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name_or_path,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=False,\n",
    "    revision=\"main\"\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name_or_path, use_fast=True)\n",
    "\n",
    "prompt = \"Write a story about llamas\"\n",
    "system_message = \"You are a story writing assistant\"\n",
    "prompt_template=f'''{prompt}\n",
    "'''\n",
    "\n",
    "print(\"\\n\\n*** Generate:\")\n",
    "\n",
    "input_ids = tokenizer(prompt_template, return_tensors='pt').input_ids.cuda()\n",
    "output = model.generate(inputs=input_ids, temperature=0.7, do_sample=True, top_p=0.95, top_k=40, max_new_tokens=512)\n",
    "print(tokenizer.decode(output[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inference can also be done using transformers' pipeline\n",
    "\n",
    "print(\"*** Pipeline:\")\n",
    "pipe = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    max_new_tokens=512,\n",
    "    do_sample=True,\n",
    "    temperature=0.7,\n",
    "    top_p=0.95,\n",
    "    top_k=40,\n",
    "    repetition_penalty=1.1\n",
    ")\n",
    "\n",
    "print(pipe(prompt_template)[0]['generated_text'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
