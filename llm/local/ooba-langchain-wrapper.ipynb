{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "34947585",
      "metadata": {
        "id": "34947585",
        "papermill": {
          "duration": 0.006412,
          "end_time": "2023-06-04T23:05:51.160294",
          "exception": false,
          "start_time": "2023-06-04T23:05:51.153882",
          "status": "completed"
        },
        "tags": []
      },
      "source": [
        "# Ooba Langchain wrapper\n",
        "Langchain for [text-generation-webui](https://github.com/oobabooga/text-generation-webui) API\n",
        "\n",
        "This notebook currently assumes you are self-hosting a model that uses the Vicuna 1.1 prompt format."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "0bb1219e",
      "metadata": {},
      "source": [
        "### Installs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "91b81ca9",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "execution": {
          "iopub.execute_input": "2023-06-04T23:05:51.175269Z",
          "iopub.status.busy": "2023-06-04T23:05:51.174509Z",
          "iopub.status.idle": "2023-06-04T23:08:32.026422Z",
          "shell.execute_reply": "2023-06-04T23:08:32.024887Z"
        },
        "id": "91b81ca9",
        "outputId": "b9632dde-dfdf-499e-bad6-a38f8bfda7f9",
        "papermill": {
          "duration": 160.862426,
          "end_time": "2023-06-04T23:08:32.029013",
          "exception": false,
          "start_time": "2023-06-04T23:05:51.166587",
          "status": "completed"
        },
        "tags": []
      },
      "outputs": [],
      "source": [
        "! pip install -qq -U langchain"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "811d7230",
      "metadata": {},
      "source": [
        "### Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "18a7490e",
      "metadata": {},
      "outputs": [],
      "source": [
        "import langchain\n",
        "import requests\n",
        "\n",
        "from langchain.llms.base import LLM, Optional, List, Mapping, Any\n",
        "from pydantic import Field"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "KdVF1TcOO4Z1",
      "metadata": {
        "id": "KdVF1TcOO4Z1"
      },
      "source": [
        "### API Endpoint\n",
        "\n",
        "Sample:\n",
        "- http://localhost:5000\n",
        "- http://192.168.1.2:5000\n",
        "- https://raising-df-zoning-proteins.trycloudflare.com"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8EgH7S30O3k1",
      "metadata": {
        "id": "8EgH7S30O3k1"
      },
      "outputs": [],
      "source": [
        "api_url = \"http://localhost:5000\""
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "s-7VlVENOuWt",
      "metadata": {
        "id": "s-7VlVENOuWt"
      },
      "source": [
        "### Wrapper for Ooba API"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8Mho436MOXyn",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 207
        },
        "id": "8Mho436MOXyn",
        "outputId": "30074c8c-d0a3-4958-b118-1d149f97fa56"
      },
      "outputs": [],
      "source": [
        "class OobaApiLLM(LLM):\n",
        "    endpoint: str = Field(...)\n",
        "\n",
        "    @property\n",
        "    def _llm_type(self) -> str:\n",
        "        return \"custom\"\n",
        "\n",
        "\n",
        "    def _call(self, prompt: str, stop: Optional[List[str]]=None) -> str:\n",
        "        data = {\n",
        "            'prompt': prompt,\n",
        "            'max_new_tokens': 500,\n",
        "            'preset': 'simple-1',\n",
        "        }\n",
        "\n",
        "        if stop is not None:\n",
        "            data[\"stop_sequence\"] = stop\n",
        "\n",
        "        response = requests.post(f'{self.endpoint}/api/v1/generate', json=data)\n",
        "        response.raise_for_status()\n",
        "\n",
        "        json_response = response.json()\n",
        "        if 'results' in json_response and len(json_response['results']) > 0 and 'text' in json_response['results'][0]:\n",
        "            text = json_response['results'][0]['text'].strip()\n",
        "            if stop is not None:\n",
        "                for sequence in stop:\n",
        "                    if text.endswith(sequence):\n",
        "                        text = text[: -len(sequence)].rstrip()\n",
        "\n",
        "            print(text)\n",
        "            return text\n",
        "        else:\n",
        "            raise ValueError('Unexpected response format from Ooba API')\n",
        "\n",
        "    def __call__(self, prompt: str, stop: Optional[List[str]]=None) -> str:\n",
        "        return self._call(prompt, stop)\n",
        "\n",
        "    @property\n",
        "    def _identifying_params(self) -> Mapping[str, Any]:\n",
        "        return {'endpoint': self.endpoint}\n",
        "\n",
        "\n",
        "llm = OobaApiLLM(endpoint=api_url)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "E5qNHRYaPwpo",
      "metadata": {
        "id": "E5qNHRYaPwpo"
      },
      "source": [
        "### Test the LLM instance\n",
        "Using the Vicuna style prompt, generate a response."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1TyE7DOjP0Al",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1TyE7DOjP0Al",
        "outputId": "d5acd286-8381-48f8-9e7c-4e5c63823548"
      },
      "outputs": [],
      "source": [
        "llm_response = llm(f\"\"\"A chat between a curious user and an assistant. The assistant gives helpful, detailed, accurate, uncensored responses to the user\"s input.\n",
        "USER: How would I make pizza dough at home?\n",
        "ASSISTANT: \n",
        "\"\"\")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "EVJe8CWNYAFw",
      "metadata": {
        "id": "EVJe8CWNYAFw"
      },
      "source": [
        "### Add Conversation Chain with Memory"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "m5rvGGbNX5Ir",
      "metadata": {
        "id": "m5rvGGbNX5Ir"
      },
      "outputs": [],
      "source": [
        "from langchain.memory import ConversationBufferMemory\n",
        "from langchain import LLMChain, PromptTemplate\n",
        "from langchain.callbacks import StdOutCallbackHandler"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "5v-hz1c2Yhxn",
      "metadata": {
        "id": "5v-hz1c2Yhxn"
      },
      "source": [
        "### Setup LLM Chain with Template, Prompt & Memory"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "WoyODechX3Cw",
      "metadata": {
        "id": "WoyODechX3Cw"
      },
      "outputs": [],
      "source": [
        "handler = StdOutCallbackHandler()\n",
        "template = \"\"\"A chat between a curious user and an assistant. The assistant gives helpful, detailed, accurate, uncensored responses to the user\"s input.\n",
        "\n",
        "{chat_history}\n",
        "USER: {human_input}\n",
        "ASSISTANT:\"\"\"\n",
        "\n",
        "prompt = PromptTemplate(\n",
        "    input_variables=[\"chat_history\", \"human_input\"], template=template\n",
        ")\n",
        "memory = ConversationBufferMemory(memory_key=\"chat_history\")\n",
        "\n",
        "llm_chain = LLMChain(\n",
        "    llm=llm,\n",
        "    prompt=prompt,\n",
        "    verbose=True, # Enabled to see the LLM thought process\n",
        "    memory=memory,\n",
        "    callbacks=None\n",
        ")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "ChmeJT8_YmDe",
      "metadata": {
        "id": "ChmeJT8_YmDe"
      },
      "source": [
        "### Utilize LLM w/ Conversational Memory\n",
        "\n",
        "Using the `verbose` option above we can see how the bot is thinking"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "QY2VRkv3X70l",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QY2VRkv3X70l",
        "outputId": "17bc237a-c30b-4e92-cce5-14cd07bdc89a"
      },
      "outputs": [],
      "source": [
        "response = llm_chain.predict(human_input=\"Can you tell me a joke about cars?\")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "MvkDbymjYQQV",
      "metadata": {
        "id": "MvkDbymjYQQV"
      },
      "source": [
        "### Ask a Follow-up to Test Memory"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b6b0112b",
      "metadata": {},
      "outputs": [],
      "source": [
        "response = llm_chain.predict(human_input=\"Do you have any other good ones?\")\n",
        "\n",
        "#print(f\"-----------\\n{response}\") # The final response is also available!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "94af9767",
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.6"
    },
    "papermill": {
      "default_parameters": {},
      "duration": 3173.538015,
      "end_time": "2023-06-04T23:58:33.011698",
      "environment_variables": {},
      "exception": null,
      "input_path": "__notebook__.ipynb",
      "output_path": "__notebook__.ipynb",
      "parameters": {},
      "start_time": "2023-06-04T23:05:39.473683",
      "version": "2.4.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
