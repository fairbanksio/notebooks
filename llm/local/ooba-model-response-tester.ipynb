{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import json\n",
    "import requests\n",
    "import time\n",
    "\n",
    "HOST = \"http://192.168.4.5:5000\"\n",
    "PROMPT = \"Where can I read the latest news?\"\n",
    "ITERATIONS = 10\n",
    "TOKENS = 1000\n",
    "\n",
    "def get_model():\n",
    "    endpoint = f\"{HOST}/api/v1/model\"\n",
    "    response = requests.request(\"GET\", endpoint, headers={}, data={})\n",
    "    response = response.json()\n",
    "    \n",
    "    return response['result']\n",
    "\n",
    "def make_llm_call():\n",
    "    endpoint = f\"{HOST}/api/v1/generate\"\n",
    "\n",
    "    payload = json.dumps({\n",
    "        \"prompt\": PROMPT,\n",
    "        \"max_new_tokens\": TOKENS,\n",
    "        \"preset\": \"simple-1\"\n",
    "    })\n",
    "\n",
    "    response = requests.request(\n",
    "        \"POST\", \n",
    "        endpoint, \n",
    "        headers={\n",
    "            'Content-Type': 'application/json'\n",
    "        }, \n",
    "        data=payload\n",
    "    )\n",
    "\n",
    "    if response.status_code != 200:\n",
    "        print(\"Error: Failed to fetch LLM response.\")\n",
    "        return\n",
    "    \n",
    "    response = response.content.decode(response.encoding)\n",
    "    decoded_response = json.loads(response)\n",
    "    decoded_response = decoded_response['results'][0]['text']\n",
    "\n",
    "    return decoded_response\n",
    "\n",
    "def main():\n",
    "    api_responses = []\n",
    "    MODEL = get_model()\n",
    "\n",
    "    print(f\"#########################################\")\n",
    "    print(f\"HOST: {HOST}\")\n",
    "    print(f\"MODEL: {MODEL}\")\n",
    "    print(f\"PROMPT: {PROMPT}\")\n",
    "    print(f\"ITERATIONS: {ITERATIONS}\")\n",
    "    print(f\"#########################################\")\n",
    "    print()\n",
    "\n",
    "    # Call Ooba API and save responses in the list\n",
    "    for i in range(ITERATIONS):\n",
    "        print(f\"> Running iteration {i + 1}\", end=\"\")\n",
    "\n",
    "        start_time = time.time()\n",
    "        llm_response = make_llm_call()\n",
    "        end_time = time.time()\n",
    "        processing_time = round(end_time - start_time, 2)\n",
    "\n",
    "        print(f\" â†’ {processing_time} seconds\")\n",
    "        print(llm_response)\n",
    "        print()\n",
    "        print()\n",
    "\n",
    "        data = {\n",
    "            \"_id\": i,\n",
    "            \"model_name\": MODEL,\n",
    "            \"user_prompt\": PROMPT,\n",
    "            \"llm_response\": llm_response,\n",
    "            \"processing_time\": processing_time\n",
    "        }\n",
    "\n",
    "        api_responses.append(data)\n",
    "\n",
    "    # Save api_responses to a JSON file\n",
    "    filename = f\"{MODEL}_{datetime.datetime.now().strftime('%Y-%m-%d_%H-%M-%S')}.json\"\n",
    "    with open(filename, \"w\") as f:\n",
    "        json.dump(api_responses, f, indent=2)\n",
    "        print()\n",
    "        print(f\"ðŸ’¾ Saved results to {filename}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
